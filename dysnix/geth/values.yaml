image:
  repository: ethereum/client-go
  pullPolicy: IfNotPresent
  tag: ""

imagePullSecrets: []
nameOverride: ""
fullnameOverride: ""

serviceAccount:
  # Specifies whether a service account should be created
  create: true
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ""

updateStrategy:
  type: RollingUpdate

## Geth v1.13+ with --state.scheme=path shuts down way faster.
terminationGracePeriodSeconds: 300

## Extra pod labels
podLabels: {}
  # environment: production

## This labels mark Geth node as ready to serve the traffic.
## Used as selector for RPC service together with `.Values.podLabels` and default labels.
podStatusLabels: {}
  # manualstatus: in-service

podSecurityContext: {}
  # fsGroup: 2000

securityContext: {}
  # capabilities:
  #   drop:
  #   - ALL
  # readOnlyRootFilesystem: true
  # runAsNonRoot: true
  # runAsUser: 1000
  # runAsGroup: 1000

## By disabling we fix "Unknown config environment variable envvar=GETH_"
## Enable if your workload depends on this functionality
enableServiceLinks: false

## Override geth container command (can be templated)
command: []

## Extra container arguments (can be templated)
extraArgs: []

## Extra init containers, can be templated
extraInitContainers: []
  # - name: dumpconfig
  #   image: "{{ .Values.image.repository }}:{{ .Values.image.tag | default .Chart.AppVersion }}"
  #   imagePullPolicy: "{{ .Values.image.pullPolicy }}"
  #   command: ["--config", "/config/config.toml", "dumpconfig"]
  #   volumeMounts:
  #   - name: config
  #     mountPath: /config

## Sidecar containers, can be templated
sidecarContainers: []
  # - name: exporter
  #   image: ethpandaops/ethereum-metrics-exporter:latest
  #   imagePullPolicy: Always
  #   args:
  #   - --execution-url=http://localhost:{{ .Values.config.node.http.port }}
  #   ports:
  #   - name: exporter
  #     containerPort: 9090

## Services config
services:
  p2p:
    enabled: true
    type: NodePort
    loadBalancerIP: ""
    port: 30303
    # it's better to set nodePort equal to .Values.config.node.p2p.port when the svc type is "NodePort"
    # nodePort: 30303
    annotations: {}
    publishNotReadyAddresses: true
  p2pDiscovery:
    enabled: true
    type: NodePort
    loadBalancerIP: ""
    port: 30301
    # it's better to set nodePort equal to .Values.config.node.p2p.port when the svc type is "NodePort"
    # nodePort: 30301
    annotations: {}
    publishNotReadyAddresses: true
  rpc:
    enabled: true
    type: ClusterIP
    httpPort: 8545
    wsPort: 8546
    annotations: {}
  metrics:
    enabled: false
    type: ClusterIP
    port: 6060
    annotations: {}
    publishNotReadyAddresses: true
  authrpc:
    enabled: true
    type: ClusterIP
    port: 8551
    annotations: {}
    publishNotReadyAddresses: true

ingress:
  http:
    enabled: false
    className: ""
    annotations: {}
      # kubernetes.io/ingress.class: nginx
      # kubernetes.io/tls-acme: "true"
      # cert-manager.io/cluster-issuer: letsencrypt-prod
    hosts: []
      # - host: geth.local
      #   paths:
      #     - path: /
      #       pathType: ImplementationSpecific
    tls: []
      # - secretName: geth-tls
      #   hosts:
      #     - geth.local
  ws:
    enabled: false
    className: ""
    annotations: {}
      # kubernetes.io/ingress.class: nginx
      # kubernetes.io/tls-acme: "true"
      # cert-manager.io/cluster-issuer: letsencrypt-prod
    hosts:
      # - host: geth-ws.local
      #   paths:
      #     - path: /
      #       pathType: ImplementationSpecific
    tls: []
      # - secretName: geth-ws-tls
      #   hosts:
      #     - geth-ws.local

persistence:
  type: pvc
  # type: hostPath
  pvc:
    size: 900Gi                # starting point for snap-synced node as of 2023-09
    accessMode: ReadWriteOnce
    storageClass: ""           # set to "-" if you want to manually create persistent volume
    annotations: {}
  hostPath:
    path: /data/geth
    type: Directory            # by default you need to create directory yourself

affinity: {}

nodeSelector: {}

tolerations: []

resources: {}
  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  # limits:
  #   cpu: 100m
  #   memory: 128Mi
  # requests:
  #   cpu: 100m
  #   memory: 128Mi

# Create Prometheus Operator serviceMonitor
serviceMonitor:
  enabled: false
  # interval: 10s
  # scrapeTimeout: 2s
  # honorLabels: true
  # relabelings: []
  # metricRelabelings: []

# .livenessProbe.exec.command can also be in a templated string format
livenessProbe:
  enabled: false
  initialDelaySeconds: 120
  periodSeconds: 60
  successThreshold: 1
  failureThreshold: 10
  timeoutSeconds: 10
  exec:
    command:
      - sh
      - /scripts/liveness.sh
      - "300"

# .readinessProbe.exec.command can also be in a templated string format
readinessProbe:
  enabled: false
  initialDelaySeconds: 60
  periodSeconds: 10
  successThreshold: 1
  failureThreshold: 2
  timeoutSeconds: 5
  exec:
    command:
      - sh
      - /scripts/readiness.sh
      - "60"

## Main Geth config
config:
  ## Use a utility like OpenSSL to create JWT via command: openssl rand -hex 32
  jwt: ""
  eth:
    network: mainnet
    syncMode: snap
    gcMode: full
    txLookupLimit: 2350000
    transactionHistory: 2350000
    stateHistory: 90000
    stateScheme: hash
    preimages: false
  node:
    ipc:
      enabled: true
      path: geth.ipc
    http:
      enabled: true
      port: 8545
      vhosts: ["*"]
      cors: ["*"]
      modules: ["eth", "net", "web3"]
    authrpc:
      port: 8551
      vhosts: ["*"]
    ws:
      enabled: false
      port: 8546
      origins: ["*"]
      modules: ["eth", "net", "web3"]
    p2p:
      useHostPort: false    # if set to true .Values.services.p2p will be disabled, and hostPort will be allocated instead
      port: 30303
      discoveryPort: 30301
      nat: ""               # you may want to set it if you are running P2P via NodePort/LoadBalancer k8s svc
      maxPeers: 50
      noDiscovery: false
      bootstrapNodes: []
      bootstrapNodesV5: []
      staticNodes: []
      trustedNodes: []
  cache: 4096
  verbosity: 3
  vmodule: []
  # - rpc=5
  metrics:
    enabled: false
    expensive: false
    port: 6060
  pprof:
    enabled: false
    port: 6061

s3config:
  image:
    repository: peakcom/s5cmd
    tag: v2.2.2
    pullPolicy: IfNotPresent
  # Any S3-compatible object storage service should be supported, but has only been tested with GCS.
  # I.e. Amazon S3, MinIO, DigitalOcean Spaces, CloudFlare R2.
  # endpointUrl: https://s3.amazonaws.com
  endpointUrl: https://storage.googleapis.com
  # Assuming your S3 bucket name is `my-snapshot-bucket` and base directory name is Helm release name
  baseUrl: my-snapshot-bucket/{{ .Release.Name }}
  # These are relative to baseUrl
  chaindataUrl: /chaindata
  ancientUrl: /ancient
  # How to create access key
  # AWS S3 https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html
  # GCS    https://cloud.google.com/storage/docs/authentication/managing-hmackeys#create
  accessKeyId: REPLACEME
  secretAccessKey: REPLACEME
  # override local paths
  # chaindataDir: /root/.ethereum/geth/chaindata
  # ancientDir: /root/.ethereum/geth/chaindata/ancient

initFromS3:
  # enable initContainer
  enabled: false
  # download snapshot from S3 on every pod start
  force: false

syncToS3:
  # enable initContainer (won't enable actual sync)
  enabled: false
  # restart pod and trigger sync to S3 inside initContainer by schedule
  cronjob:
    enabled: false
    image:
      repository: dysnix/kubectl
      tag: v1.27
      pullPolicy: IfNotPresent
    schedule: "0 2 * * *"
