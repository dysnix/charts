image:
  repository: us-docker.pkg.dev/oplabs-tools-artifacts/images/op-geth
  tag: ""
  pullPolicy: Always

imagePullSecrets: []
nameOverride: ""
fullnameOverride: ""

serviceAccount:
  # Specifies whether a service account should be created
  create: true
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ""

updateStrategy:
  type: RollingUpdate

## --state.scheme=path promised to speed up shutdown
terminationGracePeriodSeconds: 300

## Extra pod labels
podLabels: {}
  # environment: production

## This labels mark Geth node as ready to serve the traffic.
## Used as selector for RPC service together with `.Values.podLabels` and default labels.
podStatusLabels: {}
  # manualstatus: in-service

podSecurityContext: {}
  # fsGroup: 10001

securityContext:
  capabilities:
    drop:
    - ALL
  allowPrivilegeEscalation: false
  privileged: false
  # runAsNonRoot: true
  # runAsUser: 10001
  # runAsGroup: 10001
  # readOnlyRootFilesystem: true

## By disabling we fix "Unknown config environment variable envvar=GETH_"
## Enable if your workload depends on this functionality
enableServiceLinks: false

## Override op-geth command (can be templated)
command: []

## Extra op-geth arguments (can be templated)
extraArgs: []

## Extra init containers, can be templated
extraInitContainers: []
  # - name: dumpconfig
  #   image: "{{ .Values.image.repository }}:{{ .Values.image.tag | default .Chart.AppVersion }}"
  #   imagePullPolicy: "{{ .Values.image.pullPolicy }}"
  #   command: ["--config", "/config/config.toml", "dumpconfig"]
  #   volumeMounts:
  #   - name: config
  #     mountPath: /config

## Sidecar containers, can be templated
sidecarContainers: []
  # - name: replica-mon
  #   image: us-docker.pkg.dev/oplabs-tools-artifacts/images/chain-mon:v1.1.1
  #   imagePullPolicy: IfNotPresent
  #   args:
  #     - start:replica-mon
  #     - --referencerpcprovider=https://mainnet.optimism.io
  #     - --targetrpcprovider=http://localhost:{{ .Values.config.http.port }}
  #     - --hostname=0.0.0.0
  #     - --port=7300
  #   ports:
  #     - name: replica-mon
  #       containerPort: 7300
  #   readinessProbe:
  #     initialDelaySeconds: 30
  #     periodSeconds: 5
  #     successThreshold: 2
  #     failureThreshold: 3
  #     exec:
  #       command:
  #       - sh
  #       - -c
  #       - |
  #         diff=$(curl -s localhost:7300/metrics | grep "^healthcheck_height" | cut -d' ' -f2)
  #         test $diff -le 60 && exit 0 || exit 1
  #   livenessProbe:
  #     initialDelaySeconds: 60
  #     periodSeconds: 30
  #     httpGet:
  #       path: /healthz
  #       port: replica-mon

# Extra volumeMounts for op-geth container, can be templated
extraVolumeMounts: []
  # - name: testvolume
  #   mountPath: /test

# Extra volumes, can be templated
extraVolumes: []
  # - name: testvolume
  #   persistentVolumeClaim:
  #     claimName: test-pvc

## Services config
services:
  rpc:
    enabled: true
    type: ClusterIP
    httpPort: 8545
    wsPort: 8546
  authrpc:
    enabled: true
    type: ClusterIP
    port: 8551
    publishNotReadyAddresses: true
  metrics:
    enabled: true
    type: ClusterIP
    port: 6060
    publishNotReadyAddresses: true
  p2p:
    enabled: true       # disable if you are not using "snap" syncmode
    type: NodePort
    loadBalancerIP: ""
    port: 30303
    # nodePort: 30303   # exposed port of the node, must be unique across multiple node deployments
    annotations: {}
    publishNotReadyAddresses: true
  p2pDiscovery:
    enabled: true       # disable if you are not using "snap" syncmode
    type: NodePort
    loadBalancerIP: ""
    port: 30301
    # nodePort: 30301   # exposed port of the node, must be unique across multiple node deployments
    annotations: {}
    publishNotReadyAddresses: true

ingress:
  http:
    enabled: false
    className: ""
    annotations: {}
      # kubernetes.io/ingress.class: nginx
      # kubernetes.io/tls-acme: "true"
      # cert-manager.io/cluster-issuer: letsencrypt-prod
    hosts: []
      # - host: geth.local
      #   paths:
      #     - path: /
      #       pathType: ImplementationSpecific
    tls: []
      # - secretName: geth-tls
      #   hosts:
      #     - geth.local
  ws:
    enabled: false
    className: ""
    annotations: {}
      # kubernetes.io/ingress.class: nginx
      # kubernetes.io/tls-acme: "true"
      # cert-manager.io/cluster-issuer: letsencrypt-prod
    hosts:
      # - host: geth-ws.local
      #   paths:
      #     - path: /
      #       pathType: ImplementationSpecific
    tls: []
      # - secretName: geth-ws-tls
      #   hosts:
      #     - geth-ws.local

# Create Prometheus Operator serviceMonitor
serviceMonitor:
  enabled: false
  # interval: 10s
  # scrapeTimeout: 2s
  # honorLabels: true
  # relabelings: []
  # metricRelabelings: []

persistence:
  type: pvc                    # possible values are: "pvc", "hostPath"
  pvc:
    size: 2Ti                  # recommended disk size for op-mainnet full node
    accessMode: ReadWriteOnce
    storageClass: ""           # set to "-" if you want to manually create persistent volume
    annotations: {}
  hostPath:
    path: /blockchain
    type: DirectoryOrCreate    # automaticcaly create directory if it doesn't exist
  mountPath: ""                # mount path for container fs, leave blank to use value from .Values.config.datadir

affinity: {}

nodeSelector: {}

tolerations: []

resources: {}
  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  # limits:
  #   cpu: 100m
  #   memory: 128Mi
  # requests:
  #   cpu: 100m
  #   memory: 128Mi

# .startupProbe.exec.command can also be in a templated string format
startupProbe:
  enabled: false
  initialDelaySeconds: 10
  periodSeconds: 5
  failureThreshold: 120960    # periodSeconds * failureThreshold = 7 days
  timeoutSeconds: 10
  exec:
    command:
      - sh
      - /scripts/wait-for-sync.sh

# .livenessProbe.exec.command can also be in a templated string format
livenessProbe:
  enabled: false
  initialDelaySeconds: 120
  periodSeconds: 60
  successThreshold: 1
  failureThreshold: 10
  timeoutSeconds: 10
  exec:
    command:
      - sh
      - /scripts/liveness.sh
      - "300"

# .readinessProbe.exec.command can also be in a templated string format
readinessProbe:
  enabled: false
  initialDelaySeconds: 60
  periodSeconds: 10
  successThreshold: 1
  failureThreshold: 2
  timeoutSeconds: 5
  exec:
    command:
      - sh
      - /scripts/readiness.sh
      - "60"

## Main op-geth config
config:
  jwt: ""                                                   # REQUIRED for authentication with op-node
  op-network: op-mainnet                                    # must be an empty string if you are using custom genesis (f.e. when network is not in superchain-registry)
  datadir: /data                                            # data directory
  rollup:
    halt: major                                             # halt node on version mismatch, possible values: "major", "minor", "patch", "none"
    disabletxpoolgossip: true                               # tx pool gossip not supported currently, so disable it
    sequencerhttp: https://mainnet-sequencer.optimism.io/   # url of sequencer, depends on chosen network
  http:
    port: 8545
    vhosts: ["*"]
    corsdomain: ["*"]
    api: ["eth", "net", "web3"]
  ws:
    enabled: false
    port: 8546
    origins: ["*"]
    api: ["eth", "net", "web3"]
  authrpc:
    port: 8551
    vhosts: ["*"]
  state:
    scheme: path            # possible values are: "path", "hash". "hash" is going to be deprecated soon
  cache: 0                  # disable by default, let geth automatically set proper value https://blog.ethereum.org/2023/09/12/geth-v1-13-0
  syncmode: snap            # possible values are: "snap", "full"
  gcmode: full              # possible values are: "full", "archive"
  snapshot: true            # enable state snapshot generation
  maxpeers: 50              # set to 0 if you don't use "snap" syncmode
  nodiscover: false         # enable if you don't use "snap" syncmode
  port: 30303               # TCP port for P2P communication
  discovery:
    port: 30301             # UDP port for P2P discovery
  useHostPort: false        # allocate hostPorts for P2P communication instead of K8S service
  nat: "any"                # try to auto-detect node IP, refer to `geth --help` for other options
  bootnodes: []             # built-in bootnodes are used when empty
  verbosity: 3              # global log verbosity
  vmodule: []               # per-module log verbosity
  # - rpc=5
  metrics:
    enabled: false
    expensive: false

## initContainers configuration
init:
  chownData:
    enabled: false
    image:
      repository: alpine
      tag: 3.18
      pullPolicy: IfNotPresent
  genesis:
    enabled: false
    url: ""

## S3 snapshot sync config
s3config:
  image:
    repository: peakcom/s5cmd
    tag: v2.2.2
    pullPolicy: IfNotPresent
  securityContext:
    runAsUser: 1000
    runAsGroup: 1000
    runAsNonRoot: true
    capabilities:
      drop:
      - ALL
  # local storage config
  local:
    # datadir containing the state you want to upload (can be templated)
    datadir: "{{ .Values.config.datadir }}/geth/chaindata"
    # this file marks node as already initialized from snapshot
    # should be placed outside of the datadir you are uploading
    initializedFile: "{{ .Values.config.datadir }}/.initialized"
  # remote storage config
  remote:
    # Assuming your S3 bucket name is `my-snapshot-bucket` and base directory name is Helm release name
    # snapshot will be uploaded to {{ .baseUrl }}/upload directory
    baseUrl: my-snapshot-bucket/{{ .Release.Name }}
    # Any S3-compatible object storage service should be supported, but has only been tested with GCS.
    # I.e. Amazon S3, MinIO, DigitalOcean Spaces, CloudFlare R2.
    # endpointUrl: https://storage.googleapis.com
    endpointUrl: ""
    # How to create access key
    # AWS S3 https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html
    # GCS    https://cloud.google.com/storage/docs/authentication/managing-hmackeys#create
    accessKeyId: REPLACEME
    secretAccessKey: REPLACEME

initFromS3:
  # enable initContainer
  enabled: false
  # re-download snapshot from S3 on every pod start
  force: false

syncToS3:
  # enable initContainer (won't enable actual sync)
  enabled: false
  # restart pod and trigger sync to S3 inside initContainer by schedule
  cronjob:
    enabled: false
    image:
      repository: dysnix/kubectl
      tag: v1.27
      pullPolicy: IfNotPresent
    schedule: "0 2 * * *"
