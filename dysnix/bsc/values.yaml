# Default values for bsc.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

replicaCount: 1

terminationGracePeriodSeconds: 180

image:
  repository: ghcr.io/bnb-chain/bsc
  tag: ""
  pullPolicy: Always

imagePullSecrets: []
nameOverride: ""
fullnameOverride: ""

serviceAccount:
  # Specifies whether a service account should be created
  create: true
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ""

# possible values:  StatefulSet CloneSet
controller: StatefulSet

# StatefulSet-specific options
podManagementPolicy: OrderedReady

# By disabling we fix "Unknown config environment variable envvar=GETH_"
# Enable if your workload depends on this functionality
enableServiceLinks: false

# CloneSet-specific options
minReadySeconds: 60
scaleStrategy: {}
# https://openkruise.io/docs/user-manuals/cloneset#lifecycle-hook
lifecycle: {}
# scaleStrategy:
#  # scale up limit rate
#  maxUnavailable: 1
#  # selective pod deletion
#   podsToDelete:
#     - sample-9m4hp
#
# common options, default os for StatefulSet
updateStrategy:
  type: RollingUpdate

# CloneSet options example
# updateStrategy:
#   type: InPlaceOnly
#   inPlaceUpdateStrategy:
#     gracePeriodSeconds: 10
#   partitions: 3
#   maxUnavailable: 20%
#   maxSurge: 3
# .... a lot more, check https://openkruise.io/docs/user-manuals/cloneset/#update-types
podDeletionCost: 0

# Additional controller to trigger cluster autoscaler to scale up
# https://github.com/kubernetes/autoscaler/issues/2145
# w/o this there will be no scale up with [local SSD PV enabled] BSC pods hanging in Pending forever
# another option is to use hostPath volumes instead of PV & PVC
autoScaleTrigger:
  enabled: false
  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchLabels:
              bsc/role: "auto-scale-trigger"
          topologyKey: kubernetes.io/hostname
  containers:
  - name: main
    command:
      - /bin/sh
      - -c
      - sleep infinity
    image: busybox
    resources: {}
#    securityContext:
#      runAsGroup: 1001
#      runAsUser: 1001
autoScaleHelper:
  enabled: false
  image: dysnix/shell-operator:v0.0.2

continent:
# Don't use spaces or special chars
bsc:
  # we don't have other chains, it's just a pod label for now
  chain: mainnet
  # possible values: rpc, bootnode
  role: rpc
  # status label to remove node from service manually
  # possible values: in-service, any-other-value
  manualStatus: in-service
  #  Blockchain sync mode ("fast", "full", or "light") (default: fast)
  syncmode: "full"
  # Blockchain garbage collection mode ("full", "archive") (default: "full")
  gcmode: "full"
  logLevel: "info"
  base_path: "/data"
  rpcApi: ["eth", "net", "web3", "txpool"]
  rpcVhosts: ["*"]
  wsEnabled: false
  wsApi: ["net", "web3", "eth"]
  wsOrigins: ["*"]
  maxpeers: 50
  snapshot: false
  allowUnprotectedTxs: true
  cache:
    value: 8192
    preimages: true
  state:
    # possible options (1.3.x+) are "hash" and "path"
    # use empty value for 1.2.x
    scheme: ""
  db:
    # possible options (1.3.x+) are "leveldb" and "pebble"
    # use empty value for 1.2.x
    engine: ""
  txlookuplimit: null                        # WARNING: won't work on BSC >=1.3.x, node will crash if it's provided. use ".Values.history.transactions" instead
  history:
    transactions: 0                    # BSC >=1.3.x only
  pruneancient: false                     # WARNING: enabling this option is irreversible, ancient data will be permanently removed
  # extra arguments to pass to container
  extraArgs: []
  # https://github.com/bnb-chain/bsc/issues/1193
  # valid values are local, full, insecure, none
  triesVerifyMode: local
  # ignored w/o persistdiff enabled
  diffblock: 86400
  noDiscovery: false
  initGenesis: false
  initFromSnapshot: false
  initFromSnapshotImage: dysnix/zstd:v3.15.1
  initFormSnapshotPostCmd: ""
  initFromRsync: false
  initFromRsyncImage: instrumentisto/rsync-ssh:latest
  initFromGCS:
    enabled: false
    image: peakcom/s5cmd:v2.2.2
    endpoint: "https://storage.googleapis.com"
    keyID: "AWS_ACCESS_KEY_ID"
    accessKey: "AWS_SECRET_ACCESS_KEY"
    indexUrl: "bucket/path/to/file"
    baseUrlOverride: ""                   # "bucket/path/to/dir"
    fullResyncOnSrcUpdate: false
    maxUsedSpacePercent: 93               # percents
    boostStateCopyWorkers: 800            # 800 is ok for 16vCPU and 1500MB/s disk
  syncToGCS:
    enabled: false
    image: peakcom/s5cmd:v2.2.2
    endpoint: "https://storage.googleapis.com"
    keyID: "AWS_ACCESS_KEY_ID"
    accessKey: "AWS_SECRET_ACCESS_KEY"
    baseUrl: "bucket/path/to/dir"
    # should we run the cleanup every time or we may skip it sometimes? true / false
    forceCleanup: false
  prune:
    enabled: false
  pruneBlock:
    enabled: false
  forceInitFromSnapshot: false
  snapshotUrl:
  snapshotRsyncUrl: "rsync://192.168.8.4/snapshot/geth/node/geth"
  podRange: 192.168.0.0/20
  generateConfig: false
  trustedNodesSrcUrl: "gs://bucket/trusted_nodes"
  nodekeysSrcUrl: "gs://bucket/nodekeys"
  nodekeysFileName: "nodekeys"
  getNodeKey: false
  staticNodeKey: false

  # https://forum.bnbchain.org/t/try-bootnodes-after-bsc-release-v1-2-12/1998
  bootstrapNodes:
  staticNodes:
  trustedNodes:
  netrestrict:
  nodeKeys:
  readinessProbeTimestampDistinct: 300
  livenessProbeTimestampDistinct: 300
  startupProbeTimestampDistinct: 300
  metrics:
    enabled: true
    interval: 1m
    scrapeTimeout: 30s
    service:
      port: 6060
      name: bsc-metrics

# livenessProbe:
#   initialDelaySeconds: 300
#   periodSeconds: 300
#   timeoutSeconds: 10
#   successThreshold: 1
#   failureThreshold: 2

# readinessProbe:
#   initialDelaySeconds: 60
#   periodSeconds: 10
#   timeoutSeconds: 5
#   successThreshold: 1
#   failureThreshold: 3

startupProbe:
  initialDelaySeconds: 60
  periodSeconds: 10
  timeoutSeconds: 10
  successThreshold: 1
  failureThreshold: 12

service:
  # use hostPort with bsc container
  hostPortEnabled: true
  type: ClusterIP
  rpcPortName: &rpcPortName jsonrpc
  rpcPort: &rpcPort 8575
  wsPort: &wsPort 8576
  wsPortName: &wsPortName web-socket
  graphQlPort: &graphQlPort 8577
  graphQlPortName: &graphQlPortName qraphql
  p2pPort0: &p2pPort0 30311
  p2pPortName0: &p2pPortName0 p2p
  p2pPortProtocol0: &p2pPortProtocol0 TCP
  p2pPort1: &p2pPort1 30311
  p2pPortName1: &p2pPortName1 p2p-discovery
  p2pPortProtocol1: &p2pPortProtocol1 UDP
  metricsPortName: &metricsPortName metrics
  metricsPort: &metricsPort 9368
  ports:
    - port: *rpcPort
      name: *rpcPortName
    - port: *wsPort
      name: *wsPortName
    - port: *graphQlPort
      name: *graphQlPortName
    - port: *p2pPort0
      hostPort: *p2pPort0
      name: *p2pPortName0
      protocol: *p2pPortProtocol0
    - port: *p2pPort1
      hostPort: *p2pPort1
      name: *p2pPortName1
      protocol: *p2pPortProtocol1
    - port: *metricsPort
      name: *metricsPortName

logger:
  image:
    repository: ghcr.io/dysnix/docker-tiny
    tag: 0.0.1
    pullPolicy: IfNotPresent
  resources:
    requests:
      cpu: 0m
      memory: 128Mi
    limits:
      cpu: 300m
      memory: 128Mi

metrics:
  enabled: false
  interval: 1m
  scrapeTimeout: 30s
  image:
    repository: ethpandaops/ethereum-metrics-exporter
    tag: 0.21.0
    pullPolicy: IfNotPresent
  resources:
    requests:
      cpu: 0m
      memory: 128Mi
    limits:
      cpu: 500m
      memory: 256Mi

metricsExtra:
  enabled: false
  port: 9369
  portName: metricsextra
  interval: 3s
  path: /metrics
  scrapeTimeout: 3s
  args: []
    # - --url
    # - http://127.0.0.1:9369
    # - --addr
    # - ":9369"
  image:
    repository: us-docker.pkg.dev/rpcfast/geth-timestamp-diff-exporter/geth
    tag: v0.1.0
    pullPolicy: IfNotPresent
#   image:
#     repository: 31z4/ethereum-prometheus-exporter
#     tag: v1.3.0
#     pullPolicy: IfNotPresent
  resources:
    requests:
      cpu: 0m
      memory: 64Mi
    limits:
      cpu: 100m
      memory: 128Mi
  env:
    - name: GETH_PORT
      value: "8575"
    - name: LISTENER_PORT
      value: "9369"
  livenessProbe:
    httpGet:
      path: /health
      port: 9369
    initialDelaySeconds: 30
    periodSeconds: 30
    timeoutSeconds: 10
    successThreshold: 1
    failureThreshold: 2


prometheus:
  rules:
    enabled: false

rsyncd:
  enabled: false
  image: instrumentisto/rsync-ssh:latest
  bsc_path: /data
  resources:
    requests:
      cpu: 0m
      memory: 128Mi
    limits:
      cpu: 2
      memory: 1024Mi
  service:
    port: 1873
    name: rsyncd
# legacy clean up GCS using a list from sync-to-gcs
# workload identity is used
gcsCleanup:
  enabled: false
  image: google/cloud-sdk:alpine
  rmlist: rmlist.txt
  rmlog: rmlog.txt
  resources:
    requests:
      cpu: 0m
      memory: 128Mi
    limits:
      cpu: 4
      memory: 5120Mi
# updates pod annotation controller.kubernetes.io/pod-deletion-cost
updatePodDeletionCost:
  enabled: false
  image: dysnix/kubectl:v1.24
  resources:
    requests:
      cpu: 0m
      memory: 128Mi
    limits:
      cpu: 100m
      memory: 300Mi
  securityContext:
    runAsGroup: 1001
    runAsUser: 1001

# cronjobs to initiate sync-to-gcs and prune via pod restart
cronjobs:
  sync:
    enabled: false
    image: dysnix/kubectl:v1.24
    resources:
      requests:
        cpu: 0m
        memory: 128Mi
      limits:
        cpu: 100m
        memory: 300Mi
    schedule: "*/20 * * * *"
    securityContext:
      fsGroup: 1001
      runAsGroup: 1001
      runAsUser: 1001
    nodeSelector: {}
    tolerations: []
    affinity: {}
  prune:
    enabled: false
    image: dysnix/kubectl:v1.24
    resources:
      requests:
        cpu: 0m
        memory: 128Mi
      limits:
        cpu: 100m
        memory: 300Mi
    schedule: "5 3 * * 2"
    securityContext:
      fsGroup: 1001
      runAsGroup: 1001
      runAsUser: 1001
    nodeSelector: {}
    tolerations: []
    affinity: {}
  # bootnode rotation
  rotate:
    enabled: false
    image: dysnix/kubectl:v1.24
    resources:
      requests:
        cpu: 0m
        memory: 128Mi
      limits:
        cpu: 100m
        memory: 300Mi
    schedule: "5 3 * * 2"
    securityContext:
      fsGroup: 1001
      runAsGroup: 1001
      runAsUser: 1001
    nodeSelector: {}
    tolerations: []
    affinity: {}
    # specify controller and name we'll need to scale down to 0
    oldController: "statefulset/old-bootnode"
nginx:
  overrideBackendAddress: false
  corsUrl: "*"
  backendAddress:
failback:
  enabled: false
  resources:
    requests:
      cpu: 100m
      memory: 128Mi
    limits:
      cpu: 2
      memory: 256Mi
  image:
    repository: nginxinc/nginx-unprivileged
    tag: "1.20-alpine"
    pullPolicy: IfNotPresent
  port: 8000
  metrics:
    enabled: false
    image:
      repository: nginx/nginx-prometheus-exporter
      tag: "0.9.0"
      pullPolicy: IfNotPresent
    resources:
      requests:
        cpu: 0m
        memory: 128Mi
      limits:
        cpu: 300m
        memory: 256Mi
    service:
      port: 9113
      name: nginx-metrics
    interval: 1m
    scrapeTimeout: 30s

multicluster:
  enabled: false
  ingress:
    ip: ""
  service:
    port: 80
  backend:
    sessionAffinity:
      affinityCookieTtlSec: 50
      affinityType: GENERATED_COOKIE

# override is required with upstream image
bscCmdOverride: true
bscCmd: ["geth"]

externalLB: false
externalLBIP: ""
externalLBSourceRanges: {}
externalLBAnnotations: {}

externalLBp2p: false
externalLBp2pIP: ""
externalLBp2pAnnotations: {}

externalLBp2pDiscovery: false
externalLBp2pDiscoveryIP: ""
externalLBp2pDiscoveryAnnotations: {}

p2pPublishNotReadyAddresses: true

internalLB: false
internalLBIP: ""
internalLBAnnotations: {}

persistence:
  enabled: true
  storageClass: ""
  accessMode: ReadWriteOnce
  size: "1000Gi"

# hostPath is used only when persistence is disabled
hostPath:
  # emptyDir is used when both persistence and hostPath are disabled
  enabled: false
  path: /mnt/disks/raid0
  type: Directory

ingress:
  rpc:
    enabled: false
    annotations:
      {}
      # kubernetes.io/ingress.class: nginx
      # kubernetes.io/tls-acme: "true"
      # nginx.ingress.kubernetes.io/cors-allow-headers: Content-Type
      # nginx.ingress.kubernetes.io/cors-allow-methods: POST, OPTIONS
    rules:
      []
      # host: &rpcHostName rpc.example.com
    tls:
      []
      # - hosts:
      #     - *rpcHostName
      #   secretName: ssd-tls-cert
  ws:
    enabled: false
    annotations:
      {}
      # kubernetes.io/ingress.class: nginx
      # kubernetes.io/tls-acme: "true"
      # nginx.ingress.kubernetes.io/proxy-read-timeout: "3600"
      # nginx.ingress.kubernetes.io/proxy-send-timeout: "3600"
    rules:
      []
      # host: &wssHostName wss.example.com
    tls:
      []
      # - hosts:
      #     - *wssHostName
      #   secretName: ssd-tls-cert

# to pass CI tests w/o resourse requests
resources: {}
#  requests:
#    cpu: "3"
#    memory: 16Gi
#  limits:
#    memory: 16Gi

## Autoscaling parameters
autoscaling:
  enabled: false
  # scaleDownDisabled: true
  # targetCPU: 60
  # minReplicas: 1
  # maxReplicas: 10
  # targetMemory: 50

securityContext:
  fsGroup: 101
  runAsGroup: 101
  runAsUser: 101

nodeSelector: {}
tolerations: []
affinity:
  podAntiAffinity:
    preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          labelSelector:
            matchLabels:
              app.kubernetes.io/name: "bsc"
              bsc/chain: "mainnet"
          topologyKey: failure-domain.beta.kubernetes.io/zone
topologySpreadConstraints: []

podDisruptionBudget:
  enabled: false
#  minAvailable: 1
#  maxUnavailable: 1
#  unhealthyPodEvictionPolicy: AlwaysAllow
