# Default values for bsc.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

replicaCount: 1

terminationGracePeriodSeconds: 180

image:
  repository: ghcr.io/bnb-chain/bsc
  tag: ""
  pullPolicy: Always

imagePullSecrets: []
nameOverride: ""
fullnameOverride: ""

serviceAccount:
  # Specifies whether a service account should be created
  create: true
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ""

# possible values:  StatefulSet CloneSet
controller: StatefulSet

# StatefulSet-specific options
podManagementPolicy: OrderedReady

# CloneSet-specific options
minReadySeconds: 60
scaleStrategy: {}
# https://openkruise.io/docs/user-manuals/cloneset#lifecycle-hook
lifecycle: {}
# scaleStrategy:
#  # scale up limit rate
#  maxUnavailable: 1
#  # selective pod deletion
#   podsToDelete:
#     - sample-9m4hp
#
# common options, default os for StatefulSet
updateStrategy:
  type: RollingUpdate

# CloneSet options example
# updateStrategy:
#   type: InPlaceOnly
#   inPlaceUpdateStrategy:
#     gracePeriodSeconds: 10
#   partitions: 3
#   maxUnavailable: 20%
#   maxSurge: 3
# .... a lot more, check https://openkruise.io/docs/user-manuals/cloneset/#update-types
podDeletionCost: 0

# Additional controller to trigger cluster autoscaler to scale up
# https://github.com/kubernetes/autoscaler/issues/2145
# w/o this there will be no scale up with [local SSD PV enabled] BSC pods hanging in Pending forever
# another option is to use hostPath volumes instead of PV & PVC
autoScaleTrigger:
  enabled: false
  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchLabels:
              bsc/role: "auto-scale-trigger"
          topologyKey: kubernetes.io/hostname
  containers:
  - name: main
    command:
      - /bin/sh
      - -c
      - sleep infinity
    image: busybox
    resources: {}
#    securityContext:
#      runAsGroup: 1001
#      runAsUser: 1001
autoScaleHelper:
  enabled: false
  image: dysnix/shell-operator:v0.0.2

continent:
# Don't use spaces or special chars
bsc:
  # we don't have other chains, it's just a pod label for now
  chain: mainnet
  # possible values: rpc, bootnode
  role: rpc
  # status label to remove node from service manually
  # possible values: in-service, any-other-value
  manualStatus: in-service
  #  Blockchain sync mode ("fast", "full", or "light") (default: fast)
  syncmode: "full"
  # Blockchain garbage collection mode ("full", "archive") (default: "full")
  gcmode: "full"
  logLevel: "info"
  base_path: "/data"
  rpcApi: ["eth", "net", "web3", "txpool"]
  rpcVhosts: ["*"]
  wsEnabled: false
  wsApi: ["net", "web3", "eth"]
  wsOrigins: ["*"]
  maxpeers: 50
  snapshot: false
  allowUnprotectedTxs: true
  cache:
    value: 8192
    preimages: true
  state:
    # possible options (1.3.x+) are "hash" and "path"
    # use empty value for 1.2.x
    scheme: ""
  db:
    # possible options (1.3.x+) are "leveldb" and "pebble"
    # use empty value for 1.2.x
    engine: ""
  txlookuplimit: 0                        # WARNING: won't work on BSC >=1.3.x, node will crash if it's provided. use ".Values.history.transactions" instead
  history:
    transactions: null                    # BSC >=1.3.x only
  pruneancient: false                     # WARNING: enabling this option is irreversible, ancient data will be permanently removed
  # extra arguments to pass to container
  extraArgs: []
  # https://github.com/bnb-chain/bsc/issues/1193
  # valid values are local, full, insecure, none
  triesVerifyMode: local
  persistdiff: false
  # ignored w/o persistdiff enabled
  diffblock: 86400
  noDiscovery: false
  initGenesis: false
  initFromSnapshot: false
  initFromSnapshotImage: dysnix/zstd:v3.15.1
  initFormSnapshotPostCmd: ""
  initFromRsync: false
  initFromRsyncImage: instrumentisto/rsync-ssh:latest
  initFromGCS:
    enabled: false
    image: peakcom/s5cmd:v2.2.2
    endpoint: "https://storage.googleapis.com"
    keyID: "AWS_ACCESS_KEY_ID"
    accessKey: "AWS_SECRET_ACCESS_KEY"
    indexUrl: "bucket/path/to/file"
    baseUrlOverride: ""                   # "bucket/path/to/dir"
    fullResyncOnSrcUpdate: false
    maxUsedSpacePercent: 93               # percents
  syncToGCS:
    enabled: false
    image: peakcom/s5cmd:v2.2.2
    endpoint: "https://storage.googleapis.com"
    keyID: "AWS_ACCESS_KEY_ID"
    accessKey: "AWS_SECRET_ACCESS_KEY"
    baseUrl: "bucket/path/to/dir"
    # should we run the cleanup every time or we may skip it sometimes? true / false
    forceCleanup: false
  prune:
    enabled: false
  pruneBlock:
    enabled: false
  forceInitFromSnapshot: false
  snapshotUrl:
  snapshotRsyncUrl: "rsync://192.168.8.4/snapshot/geth/node/geth"
  podRange: 192.168.0.0/20
  generateConfig: false
  trustedNodesSrcUrl: "gs://bucket/trusted_nodes"
  nodekeysSrcUrl: "gs://bucket/nodekeys"
  nodekeysFileName: "nodekeys"
  getNodeKey: false
  staticNodeKey: false

  bootstrapNodes:
    - "enode://1cc4534b14cfe351ab740a1418ab944a234ca2f702915eadb7e558a02010cb7c5a8c295a3b56bcefa7701c07752acd5539cb13df2aab8ae2d98934d712611443@52.71.43.172:30311"
    - "enode://28b1d16562dac280dacaaf45d54516b85bc6c994252a9825c5cc4e080d3e53446d05f63ba495ea7d44d6c316b54cd92b245c5c328c37da24605c4a93a0d099c4@34.246.65.14:30311"
    - "enode://5a7b996048d1b0a07683a949662c87c09b55247ce774aeee10bb886892e586e3c604564393292e38ef43c023ee9981e1f8b335766ec4f0f256e57f8640b079d5@35.73.137.11:30311"
  staticNodes:
    - "enode://f3cfd69f2808ef64838abd8786342c0b22fdd28268703c8d6812e26e109f9a7cb2b37bd49724ebb46c233289f22da82991c87345eb9a2dadeddb8f37eeb259ac@18.180.28.21:30311"
    - "enode://ae74385270d4afeb953561603fcedc4a0e755a241ffdea31c3f751dc8be5bf29c03bf46e3051d1c8d997c45479a92632020c9a84b96dcb63b2259ec09b4fde38@54.178.30.104:30311"
    - "enode://d1cabe083d5fc1da9b510889188f06dab891935294e4569df759fc2c4d684b3b4982051b84a9a078512202ad947f9240adc5b6abea5320fb9a736d2f6751c52e@54.238.28.14:30311"
    - "enode://f420209bac5324326c116d38d83edfa2256c4101a27cd3e7f9b8287dc8526900f4137e915df6806986b28bc79b1e66679b544a1c515a95ede86f4d809bd65dab@54.178.62.117:30311"
    - "enode://c0e8d1abd27c3c13ca879e16f34c12ffee936a7e5d7b7fb6f1af5cc75c6fad704e5667c7bbf7826fcb200d22b9bf86395271b0f76c21e63ad9a388ed548d4c90@54.65.247.12:30311"
    - "enode://f1b49b1cf536e36f9a56730f7a0ece899e5efb344eec2fdca3a335465bc4f619b98121f4a5032a1218fa8b69a5488d1ec48afe2abda073280beec296b104db31@13.114.199.41:30311"
    - "enode://4924583cfb262b6e333969c86eab8da009b3f7d165cc9ad326914f576c575741e71dc6e64a830e833c25e8c45b906364e58e70cdf043651fd583082ea7db5e3b@18.180.17.171:30311"
    - "enode://4d041250eb4f05ab55af184a01aed1a71d241a94a03a5b86f4e32659e1ab1e144be919890682d4afb5e7afd837146ce584d61a38837553d95a7de1f28ea4513a@54.178.99.222:30311"
    - "enode://b5772a14fdaeebf4c1924e73c923bdf11c35240a6da7b9e5ec0e6cbb95e78327690b90e8ab0ea5270debc8834454b98eca34cc2a19817f5972498648a6959a3a@54.170.158.102:30311"
    - "enode://f329176b187cec87b327f82e78b6ece3102a0f7c89b92a5312e1674062c6e89f785f55fb1b167e369d71c66b0548994c6035c6d85849eccb434d4d9e0c489cdd@34.253.94.130:30311"
    - "enode://cbfd1219940d4e312ad94108e7fa3bc34c4c22081d6f334a2e7b36bb28928b56879924cf0353ad85fa5b2f3d5033bbe8ad5371feae9c2088214184be301ed658@54.75.11.3:30311"
    - "enode://c64b0a0c619c03c220ea0d7cac754931f967665f9e148b92d2e46761ad9180f5eb5aaef48dfc230d8db8f8c16d2265a3d5407b06bedcd5f0f5a22c2f51c2e69f@54.216.208.163:30311"
    - "enode://352a361a9240d4d23bb6fab19cc6dc5a5fc6921abf19de65afe13f1802780aecd67c8c09d8c89043ff86947f171d98ab06906ef616d58e718067e02abea0dda9@79.125.105.65:30311"
    - "enode://bb683ef5d03db7d945d6f84b88e5b98920b70aecc22abed8c00d6db621f784e4280e5813d12694c7a091543064456ad9789980766f3f1feb38906cf7255c33d6@54.195.127.237:30311"
    - "enode://11dc6fea50630b68a9289055d6b0fb0e22fb5048a3f4e4efd741a7ab09dd79e78d383efc052089e516f0a0f3eacdd5d3ffbe5279b36ecc42ad7cd1f2767fdbdb@46.137.182.25:30311"
    - "enode://21530e423b42aed17d7eef67882ebb23357db4f8b10c94d4c71191f52955d97dc13eec03cfeff0fe3a1c89c955e81a6970c09689d21ecbec2142b26b7e759c45@54.216.119.18:30311"
    - "enode://d61a31410c365e7fcd50e24d56a77d2d9741d4a57b295cc5070189ad90d0ec749d113b4b0432c6d795eb36597efce88d12ca45e645ec51b3a2144e1c1c41b66a@34.204.129.242:30311"
    - "enode://bb91215b1d77c892897048dd58f709f02aacb5355aa8f50f00b67c879c3dffd7eef5b5a152ac46cdfb255295bec4d06701a8032456703c6b604a4686d388ea8f@75.101.197.198:30311"
    - "enode://786acbdf5a3cf91b99047a0fd8305e11e54d96ea3a72b1527050d3d6f8c9fc0278ff9ef56f3e56b3b70a283d97c309065506ea2fc3eb9b62477fd014a3ec1a96@107.23.90.162:30311"
    - "enode://4653bc7c235c3480968e5e81d91123bc67626f35c207ae4acab89347db675a627784c5982431300c02f547a7d33558718f7795e848d547a327abb111eac73636@54.144.170.236:30311"
    - "enode://c6ffd994c4ef130f90f8ee2fc08c1b0f02a6e9b12152092bf5a03dd7af9fd33597d4b2e2000a271cc0648d5e55242aeadd6d5061bb2e596372655ba0722cc704@54.147.151.108:30311"
    - "enode://99b07e9dc5f204263b87243146743399b2bd60c98f68d1239a3461d09087e6c417e40f1106fa606ccf54159feabdddb4e7f367559b349a6511e66e525de4906e@54.81.225.170:30311"
    - "enode://1479af5ea7bda822e8747d0b967309bced22cad5083b93bc6f4e1d7da7be067cd8495dc4c5a71579f2da8d9068f0c43ad6933d2b335a545b4ae49a846122b261@52.7.247.132:30311"
    - "enode://43562d35f274d9e93f5ccac484c7cb185eabc746dbc9f3a56c36dc5a9ef05a3282695de7694a71c0bf4600651f49395b2ee7a6aaef857db2ac896e0fcbe6b518@35.73.15.198:30311"
    - "enode://08867e57849456fc9b0b00771f53e87ca6f2dd618c23b34a35d0c851cd484a4b7137905c5b357795025b368e4f8fe4c841b752b0c28cc2dbbf41a03d048e0e24@35.74.39.234:30311"
  trustedNodes:
  netrestrict:
  nodeKeys:
  readinessProbeTimestampDistinct: 300
  livenessProbeTimestampDistinct: 300
  startupProbeTimestampDistinct: 300
  metrics:
    enabled: true
    interval: 1m
    scrapeTimeout: 30s
    service:
      port: 6060
      name: bsc-metrics

# livenessProbe:
#   initialDelaySeconds: 300
#   periodSeconds: 300
#   timeoutSeconds: 10
#   successThreshold: 1
#   failureThreshold: 2

# readinessProbe:
#   initialDelaySeconds: 60
#   periodSeconds: 10
#   timeoutSeconds: 5
#   successThreshold: 1
#   failureThreshold: 3

startupProbe:
  initialDelaySeconds: 60
  periodSeconds: 10
  timeoutSeconds: 10
  successThreshold: 1
  failureThreshold: 12

service:
  # use hostPort with bsc container
  hostPortEnabled: true
  type: ClusterIP
  rpcPortName: &rpcPortName jsonrpc
  rpcPort: &rpcPort 8575
  wsPort: &wsPort 8576
  wsPortName: &wsPortName web-socket
  graphQlPort: &graphQlPort 8577
  graphQlPortName: &graphQlPortName qraphql
  p2pPort0: &p2pPort0 30311
  p2pPortName0: &p2pPortName0 p2p
  p2pPortProtocol0: &p2pPortProtocol0 TCP
  p2pPort1: &p2pPort1 30311
  p2pPortName1: &p2pPortName1 p2p-discovery
  p2pPortProtocol1: &p2pPortProtocol1 UDP
  metricsPortName: &metricsPortName metrics
  metricsPort: &metricsPort 9368
  ports:
    - port: *rpcPort
      name: *rpcPortName
    - port: *wsPort
      name: *wsPortName
    - port: *graphQlPort
      name: *graphQlPortName
    - port: *p2pPort0
      hostPort: *p2pPort0
      name: *p2pPortName0
      protocol: *p2pPortProtocol0
    - port: *p2pPort1
      hostPort: *p2pPort1
      name: *p2pPortName1
      protocol: *p2pPortProtocol1
    - port: *metricsPort
      name: *metricsPortName

logger:
  image:
    repository: krallin/ubuntu-tini
    tag: latest
    pullPolicy: IfNotPresent
  resources:
    requests:
      cpu: 0m
      memory: 128Mi
    limits:
      cpu: 300m
      memory: 128Mi

metrics:
  enabled: false
  interval: 1m
  scrapeTimeout: 30s
  image:
    repository: ethpandaops/ethereum-metrics-exporter
    tag: 0.21.0
    pullPolicy: IfNotPresent
  resources:
    requests:
      cpu: 0m
      memory: 128Mi
    limits:
      cpu: 500m
      memory: 256Mi

metricsExtra:
  enabled: false
  port: 9369
  portName: metricsextra
  interval: 3s
  path: /metrics
  scrapeTimeout: 3s
  args: []
    # - --url
    # - http://127.0.0.1:9369
    # - --addr
    # - ":9369"
  image:
    repository: us-docker.pkg.dev/rpcfast/geth-timestamp-diff-exporter/geth
    tag: v0.1.0
    pullPolicy: IfNotPresent
#   image:
#     repository: 31z4/ethereum-prometheus-exporter
#     tag: v1.3.0
#     pullPolicy: IfNotPresent
  resources:
    requests:
      cpu: 0m
      memory: 64Mi
    limits:
      cpu: 100m
      memory: 128Mi
  env:
    - name: GETH_PORT
      value: "8575"
    - name: LISTENER_PORT
      value: "9369"

prometheus:
  rules:
    enabled: false

rsyncd:
  enabled: false
  image: instrumentisto/rsync-ssh:latest
  bsc_path: /data
  resources:
    requests:
      cpu: 0m
      memory: 128Mi
    limits:
      cpu: 2
      memory: 1024Mi
  service:
    port: 1873
    name: rsyncd
# legacy clean up GCS using a list from sync-to-gcs
# workload identity is used
gcsCleanup:
  enabled: false
  image: google/cloud-sdk:alpine
  rmlist: rmlist.txt
  rmlog: rmlog.txt
  resources:
    requests:
      cpu: 0m
      memory: 128Mi
    limits:
      cpu: 4
      memory: 5120Mi
# updates pod annotation controller.kubernetes.io/pod-deletion-cost
updatePodDeletionCost:
  enabled: false
  image: dysnix/kubectl:v1.24
  resources:
    requests:
      cpu: 0m
      memory: 128Mi
    limits:
      cpu: 100m
      memory: 300Mi
  securityContext:
    runAsGroup: 1001
    runAsUser: 1001

# cronjobs to initiate sync-to-gcs and prune via pod restart
cronjobs:
  sync:
    enabled: false
    image: dysnix/kubectl:v1.24
    resources:
      requests:
        cpu: 0m
        memory: 128Mi
      limits:
        cpu: 100m
        memory: 300Mi
    schedule: "*/20 * * * *"
    securityContext:
      fsGroup: 1001
      runAsGroup: 1001
      runAsUser: 1001
    nodeSelector: {}
    tolerations: []
    affinity: {}
  prune:
    enabled: false
    image: dysnix/kubectl:v1.24
    resources:
      requests:
        cpu: 0m
        memory: 128Mi
      limits:
        cpu: 100m
        memory: 300Mi
    schedule: "5 3 * * 2"
    securityContext:
      fsGroup: 1001
      runAsGroup: 1001
      runAsUser: 1001
    nodeSelector: {}
    tolerations: []
    affinity: {}
  # bootnode rotation
  rotate:
    enabled: false
    image: dysnix/kubectl:v1.24
    resources:
      requests:
        cpu: 0m
        memory: 128Mi
      limits:
        cpu: 100m
        memory: 300Mi
    schedule: "5 3 * * 2"
    securityContext:
      fsGroup: 1001
      runAsGroup: 1001
      runAsUser: 1001
    nodeSelector: {}
    tolerations: []
    affinity: {}
    # specify controller and name we'll need to scale down to 0
    oldController: "statefulset/old-bootnode"
nginx:
  overrideBackendAddress: false
  corsUrl: "*"
  backendAddress:
failback:
  enabled: false
  resources:
    requests:
      cpu: 100m
      memory: 128Mi
    limits:
      cpu: 2
      memory: 256Mi
  image:
    repository: nginxinc/nginx-unprivileged
    tag: "1.20-alpine"
    pullPolicy: IfNotPresent
  port: 8000
  metrics:
    enabled: false
    image:
      repository: nginx/nginx-prometheus-exporter
      tag: "0.9.0"
      pullPolicy: IfNotPresent
    resources:
      requests:
        cpu: 0m
        memory: 128Mi
      limits:
        cpu: 300m
        memory: 256Mi
    service:
      port: 9113
      name: nginx-metrics
    interval: 1m
    scrapeTimeout: 30s

multicluster:
  enabled: false
  ingress:
    ip: ""
  service:
    port: 80
  backend:
    sessionAffinity:
      affinityCookieTtlSec: 50
      affinityType: GENERATED_COOKIE

# override is required with upstream image
bscCmdOverride: true
bscCmd: ["geth"]

externalLB: false
externalLBIP: ""
externalLBSourceRanges: {}
externalLBAnnotations: {}

externalLBp2p: false
externalLBp2pIP: ""
externalLBp2pAnnotations: {}

externalLBp2pDiscovery: false
externalLBp2pDiscoveryIP: ""
externalLBp2pDiscoveryAnnotations: {}

p2pPublishNotReadyAddresses: true

internalLB: false
internalLBIP: ""
internalLBAnnotations: {}

persistence:
  enabled: true
  storageClass: ""
  accessMode: ReadWriteOnce
  size: "1000Gi"

# hostPath is used only when persistence is disabled
hostPath:
  # emptyDir is used when both persistence and hostPath are disabled
  enabled: false
  path: /mnt/disks/raid0
  type: Directory

ingress:
  rpc:
    enabled: false
    annotations:
      {}
      # kubernetes.io/ingress.class: nginx
      # kubernetes.io/tls-acme: "true"
      # nginx.ingress.kubernetes.io/cors-allow-headers: Content-Type
      # nginx.ingress.kubernetes.io/cors-allow-methods: POST, OPTIONS
    rules:
      []
      # host: &rpcHostName rpc.example.com
    tls:
      []
      # - hosts:
      #     - *rpcHostName
      #   secretName: ssd-tls-cert
  ws:
    enabled: false
    annotations:
      {}
      # kubernetes.io/ingress.class: nginx
      # kubernetes.io/tls-acme: "true"
      # nginx.ingress.kubernetes.io/proxy-read-timeout: "3600"
      # nginx.ingress.kubernetes.io/proxy-send-timeout: "3600"
    rules:
      []
      # host: &wssHostName wss.example.com
    tls:
      []
      # - hosts:
      #     - *wssHostName
      #   secretName: ssd-tls-cert

# to pass CI tests w/o resourse requests
resources: {}
#  requests:
#    cpu: "3"
#    memory: 16Gi
#  limits:
#    memory: 16Gi

## Autoscaling parameters
autoscaling:
  enabled: false
  # scaleDownDisabled: true
  # targetCPU: 60
  # minReplicas: 1
  # maxReplicas: 10
  # targetMemory: 50

securityContext:
  fsGroup: 101
  runAsGroup: 101
  runAsUser: 101

nodeSelector: {}
tolerations: []
affinity:
  podAntiAffinity:
    preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          labelSelector:
            matchLabels:
              app.kubernetes.io/name: "bsc"
              bsc/chain: "mainnet"
          topologyKey: failure-domain.beta.kubernetes.io/zone
topologySpreadConstraints: []

podDisruptionBudget:
  enabled: false
#  minAvailable: 1
#  maxUnavailable: 1
#  unhealthyPodEvictionPolicy: AlwaysAllow
